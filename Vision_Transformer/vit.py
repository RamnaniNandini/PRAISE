# -*- coding: utf-8 -*-
"""VIT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i42CsqYQPZOGIZD8ysl3fe8zmstktVM4
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torchvision.models import resnet18
from sklearn.metrics import accuracy_score

# Define the Siamese Network
class SiameseNetwork(nn.Module):
    def __init__(self, backbone):
        super(SiameseNetwork, self).__init__()
        self.backbone = backbone

    def forward_one(self, x):
        return self.backbone(x)

    def forward(self, input1, input2):
        output1 = self.forward_one(input1)
        output2 = self.forward_one(input2)
        return output1, output2

# Define the Vision Transformer backbone
class VisionTransformer(nn.Module):
    #def __init__(self, num_classes):
        #super(VisionTransformer, self).__init__()
    def __init__(self, image_size, patch_size, num_classes, dim, num_heads, num_layers):
        super(VisionTransformer, self).__init__()
        num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size * patch_size  # Assuming 3-channel images

        self.patch_embed = nn.Conv2d(in_channels=3, out_channels=dim, kernel_size=patch_size, stride=patch_size)

        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads),
            num_layers=num_layers
        )

        self.fc = nn.Linear(dim * num_patches, num_classes)
        # Define your Vision Transformer layers here

    def forward(self, x):
        x = self.patch_embed(x)
        x = x.flatten(2).transpose(1, 2)  # Reshape for transformer

        x = self.transformer_encoder(x)
        x = x.transpose(0, 1).reshape(x.size(1), -1)

        x = self.fc(x)
        return x

# Define your dataset class and transformations
# Make sure to load pairs of images for training the Siamese Network
import os
from PIL import Image
from torch.utils.data import Dataset

class SiameseDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.personality_folders = os.listdir(root_dir)

    def __len__(self):
        return len(self.personality_folders)

    def __getitem__(self, idx):
        personality_folder = self.personality_folders[idx]
        personality_path = os.path.join(self.root_dir, personality_folder)
        image_names = os.listdir(personality_path)

        # Load a pair of images for siamese training
        # Modify this part to load the appropriate images and labels
        image1 = Image.open(os.path.join(personality_path, image_names[0]))
        image2 = Image.open(os.path.join(personality_path, image_names[1]))

        if self.transform:
            image1 = self.transform(image1)
            image2 = self.transform(image2)

        min_height = min(image1.shape[1], image2.shape[1])
        min_width = min(image1.shape[2], image2.shape[2])

        image1 = image1[:, :min_height, :min_width]
        image2 = image2[:, :min_height, :min_width]

        label = torch.tensor(1 if personality_folder == "positive_personality" else 0)
        return image1, image2, label

transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Adjust to the desired image size
    transforms.ToTensor()
])

#train_dataset = SiameseDataset(root_dir='path_to_train_dataset_folder', transform=transform)
#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
#In this example, the transforms.Resize() operation ensures that all images in a batch are of the same dimensions before stacking, which should resolve the RuntimeError issue you were encountering.







# Example usage
train_dataset = SiameseDataset(root_dir='/content/drive/MyDrive/dataset/training_set', transform=transforms.ToTensor())
train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)

test_dataset = SiameseDataset(root_dir='/content/drive/MyDrive/dataset/test_set', transform=transforms.ToTensor())
test_loader = DataLoader(test_dataset, batch_size=15, shuffle=False)

#test_dataset = YourCustomDataset(...)
#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the dataset
#train_dataset = /content/drive/MyDrive/dataset
#train_loader = DataLoader(train_dataset, batch_size=15, shuffle=True)

# Initialize the Siamese Network with Vision Transformer backbone
# Specify the parameters for the Vision Transformer
#image_size = 224  # Change this to match your image size
#patch_size = 16   # Change this to the desired patch size
#dim = 256         # Model's inner dimension
#num_heads = 8     # Number of attention heads
#num_layers = 6    # Number of layers

# Initialize the Vision Transformer backbone
backbone = VisionTransformer(image_size, patch_size, num_classes=128, dim=dim, num_heads=num_heads, num_layers=num_layers)
siamese_net = SiameseNetwork(backbone)

# Define loss function and optimizer
criterion = nn.CosineEmbeddingLoss()
optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)




#ground_truth_labels = [1, 0, 1, 0, ...]

num_epochs = 15
# Training loop
for epoch in range(num_epochs):
    for batch_idx, (anchor, positive, label) in enumerate(train_loader):
        optimizer.zero_grad()
        output1, output2 = siamese_net(anchor, positive)
        loss = criterion(output1, output2, label)
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}")

# Testing loop
siamese_net.eval()
test_loss = 0
predictions = []
threshold = 0.5
with torch.no_grad():
    for batch_idx, (anchor, positive, label) in enumerate(test_loader):
        output1, output2 = siamese_net(anchor, positive)
        test_loss += criterion(output1, output2, label).item()
        predictions.extend((output1 - output2).norm(dim=1).cpu().numpy() < threshold)

# Calculate accuracy
correct_predictions = sum(1 for pred, truth in zip(predictions, label) if pred == truth)
total_predictions = len(label)
accuracy = correct_predictions / total_predictions

print(f"Test Loss: {test_loss}, Accuracy: {accuracy}")











































# Define the Siamese Network
class SiameseNetwork(nn.Module):
    def __init__(self, vit_model):
        super(SiameseNetwork, self).__init__()
        self.vit_model = vit_model
        self.fc = nn.Sequential(
            nn.Linear(768, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 64),
            nn.ReLU(inplace=True),
            nn.Linear(64, 32))

    def forward_one(self, x):
        x = self.vit_model(x).last_hidden_state.mean(1)  # Global average pooling
        x = self.fc(x)
        return x

    def forward(self, input1, input2):
        output1 = self.forward_one(input1)
        output2 = self.forward_one(input2)
        return output1, output2

# Define a custom dataset
class CustomDataset(Dataset):
    def __init__(self, image_folder, image_list, transform=None):
        self.image_folder = image_folder
        self.image_list = image_list
        self.transform = transform

    def __len__(self):
        return len(self.image_list)

    def __getitem__(self, idx):
        img_name = os.path.join(self.image_folder, self.image_list[idx])
        image = Image.open(img_name).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image

# Set hyperparameters
batch_size = 32
learning_rate = 0.001
num_epochs = 10

# Initialize ViT model
vit_model_name = 'google/vit-base-patch16-224-in21k'
vit_feature_extractor = ViTFeatureExtractor(model_name=vit_model_name)
vit_model = ViTModel.from_pretrained(vit_model_name)

# Create Siamese Network model
siamese_net = SiameseNetwork(vit_model)

# Define transforms for image preprocessing
transform = transforms.Compose([transforms.Resize((224, 224)),
                                transforms.ToTensor()])

# Load the dataset
image_folder = '/content/drive/MyDrive/dataset/training_set/Agreeableness'
image_list = os.listdir(image_folder)

# Split dataset into train and test
train_size = int(0.8 * len(image_list))
train_images = image_list[:train_size]
test_images = image_list[train_size:]

train_dataset = CustomDataset(image_folder, train_images, transform=transform)
test_dataset = CustomDataset(image_folder, test_images, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define loss function and optimizer
criterion = nn.TripletMarginLoss(margin=1.0)
optimizer = optim.Adam(siamese_net.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    siamese_net.train()
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs = data
        optimizer.zero_grad()
        anchor, positive = inputs[0], inputs[1]
        anchor_output, positive_output = siamese_net(anchor, positive)
        loss = criterion(anchor_output, positive_output, torch.ones(anchor_output.size(0)).cuda())
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}')

# Evaluation
siamese_net.eval()
correct = 0
total = 0

with torch.no_grad():
    for i, data in enumerate(test_loader, 0):
        inputs = data
        anchor, positive = inputs[0], inputs[1]
        anchor_output, positive_output = siamese_net(anchor, positive)
        euclidean_distance = F.pairwise_distance(anchor_output, positive_output)
        # You can define a threshold to determine if two images are similar or not
        threshold = 0.5
        predictions = euclidean_distance < threshold
        total += anchor.size(0)
        correct += (predictions == True).sum().item()

print(f'Accuracy on the test set: {(100 * correct / total):.2f}%')